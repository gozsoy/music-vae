{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import plot_piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# group according to bar length\n",
    "# do hidden state concat or not in decoder bottom lstm cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable declarations\n",
    "\n",
    "TEMPO = 120.  # 1 sec = 2 beats\n",
    "SIXTEENTH_NOTE_BEATS = 0.25  # 1 16th note = 0.25 beats\n",
    "SIXTEENTH_NOTE_LEN = SIXTEENTH_NOTE_BEATS / (TEMPO / 60.)  # 1 16th note = 0.125 sec\n",
    "ONE_BAR_LEN = SIXTEENTH_NOTE_LEN * 16  # 16 16th notes = 1 bar = 2 sec\n",
    "FOUR_BAR_LEN = ONE_BAR_LEN * 4  # 16-bar = 32 sec\n",
    "\n",
    "EVENT_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function definitions\n",
    "\n",
    "def midi_notes2notes_df(notes):\n",
    "\n",
    "    prev_note = notes[0]\n",
    "\n",
    "    processed_notes = []\n",
    "    for temp_note in notes:\n",
    "        pitch = temp_note.pitch\n",
    "        duration = temp_note.end - temp_note.start\n",
    "        step = temp_note.start - prev_note.start\n",
    "        prev_note = temp_note\n",
    "\n",
    "        processed_notes.append({'pitch':pitch,'duration':duration,'step':step})\n",
    "\n",
    "    notes_df = pd.DataFrame.from_dict(processed_notes)\n",
    "\n",
    "    return notes_df\n",
    "\n",
    "\n",
    "def notesdf2midi_notes(notes_df):\n",
    "\n",
    "    recovered_midi_notes = []\n",
    "    current_step = 0.0\n",
    "    for _,row in notes_df.iterrows():\n",
    "        note_duration = row['duration']\n",
    "        current_step = current_step + row['step']\n",
    "        recovered_midi_notes.append(pretty_midi.Note(velocity=100,pitch=int(row['pitch']),start=current_step,end=current_step+note_duration))\n",
    "    \n",
    "    return recovered_midi_notes\n",
    "\n",
    "\n",
    "# reconstruction of recovered notes\n",
    "def pred_df2midi_file(notes_df):\n",
    "    recovered_notes = notesdf2midi_notes(notes_df.iloc[:64])\n",
    "\n",
    "    pm = pretty_midi.Instrument(program=0,is_drum=False)\n",
    "    pm.notes = recovered_notes\n",
    "\n",
    "    recovered_midi_file = pretty_midi.PrettyMIDI(initial_tempo=120.)\n",
    "    recovered_midi_file.instruments = [pm]\n",
    "    recovered_midi_file.time_signature_changes = [pretty_midi.TimeSignature(4,4,0.0)]\n",
    "    recovered_midi_file.write('../data/processed/mini_guitar.mid')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_data = pretty_midi.PrettyMIDI(midi_file='../data/midi_dump/turkish.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_data2tensor(midi_data):\n",
    "    notes = midi_data.instruments[0].notes\n",
    "    notes_df = midi_notes2notes_df(notes)\n",
    "    split_indices = np.arange(start=EVENT_SIZE,stop=len(notes_df),step=EVENT_SIZE)\n",
    "    single_batch_of_events = np.stack(np.split(notes_df.values,split_indices,axis=0)[:-1],axis=0)\n",
    "    return single_batch_of_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=3'>4</a>\u001b[0m complete_batch_of_events \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m midi_file \u001b[39min\u001b[39;00m midi_list:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=6'>7</a>\u001b[0m     midi_data \u001b[39m=\u001b[39m pretty_midi\u001b[39m.\u001b[39mPrettyMIDI(midi_file)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=7'>8</a>\u001b[0m     single_batch_of_events \u001b[39m=\u001b[39m midi_data2tensor(midi_data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m complete_batch_of_events \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py:69\u001b[0m, in \u001b[0;36mPrettyMIDI.__init__\u001b[0;34m(self, midi_file, resolution, initial_tempo)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=66'>67</a>\u001b[0m     tick \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=67'>68</a>\u001b[0m     \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m track:\n\u001b[0;32m---> <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=68'>69</a>\u001b[0m         event\u001b[39m.\u001b[39mtime \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tick\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=69'>70</a>\u001b[0m         tick \u001b[39m=\u001b[39m event\u001b[39m.\u001b[39mtime\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=71'>72</a>\u001b[0m \u001b[39m# Store the resolution for later use\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "midi_list = glob.glob(\"../data/maestro-v3.0.0/2018/*.midi\")\n",
    "\n",
    "complete_batch_of_events = None\n",
    "\n",
    "for midi_file in midi_list:\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    single_batch_of_events = midi_data2tensor(midi_data)\n",
    "    if complete_batch_of_events is None:\n",
    "        complete_batch_of_events = single_batch_of_events\n",
    "    else:\n",
    "        complete_batch_of_events= np.concatenate((complete_batch_of_events,single_batch_of_events),axis=0)\n",
    "\n",
    "print(complete_batch_of_events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(complete_batch_of_events)\n",
    "train_ds = train_ds.shuffle(1000).batch(6)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_ds in train_ds:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE DIMENSION OF X : 128 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,LSTMCell\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "CONDUCTOR_LEN = 4\n",
    "\n",
    "\n",
    "class MusicVAE_Decoder(tf.keras.Model):\n",
    "    def __init__(self,latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.conductor_dense = Dense(units=512,activation='tanh')\n",
    "        self.conductor_lstm_1 = LSTM(units=512,return_sequences=True)\n",
    "        self.conductor_lstm_2 = LSTM(units=256,return_sequences=True)\n",
    "        self.bottom_lstm_input_dense = Dense(units=32,activation='tanh')\n",
    "        self.bottom_lstm_cell = LSTMCell(units=32)\n",
    "        self.bottom_lstm_pitch_dense = Dense(units=128)\n",
    "        self.bottom_lstm_duration_dense = Dense(units=1)\n",
    "        self.bottom_lstm_step_dense = Dense(units=1)\n",
    "\n",
    "    def call(self,z,x,teacher_forcing=True): # teacher_forcing = False when prediction\n",
    "\n",
    "        # CONDUCTOR RNN\n",
    "        conductor_rnn_h0 = self.conductor_dense(z)\n",
    "        batch_size,_ = conductor_rnn_h0.shape\n",
    "        conductor_input = tf.zeros(shape=(batch_size,CONDUCTOR_LEN,1))\n",
    "        conductor_output = self.conductor_lstm_1(inputs=conductor_input, initial_state=[conductor_rnn_h0, conductor_rnn_h0])\n",
    "        conductor_output = self.conductor_lstm_2(conductor_output)\n",
    "        bottom_input = self.bottom_lstm_input_dense(conductor_output) \n",
    "\n",
    "        # BOTTOM RNN\n",
    "        total_seq_len = x.shape[1]\n",
    "        subseq_len = int(total_seq_len/CONDUCTOR_LEN)\n",
    "        \n",
    "        pred = None\n",
    "        temp_pred = tf.zeros_like(x[:, 0, :])\n",
    "        for subsec_idx in range(CONDUCTOR_LEN):\n",
    "            bottom_rnn_h0 = bottom_input[:,subsec_idx,:]\n",
    "            subseq_x = x[:,subsec_idx*subseq_len:(subsec_idx+1)*subseq_len,:]\n",
    "\n",
    "            h_next,c_next,subseq_pred = None,None,None\n",
    "            for j in range(0, subseq_x.shape[1]):\n",
    "                if j == 0:\n",
    "                    if teacher_forcing:\n",
    "                        _, (h_n, c_n) = self.bottom_lstm_cell(inputs=tf.zeros_like(subseq_x[:, j, :]), states=[bottom_rnn_h0,bottom_rnn_h0])\n",
    "                    else:\n",
    "                        _, (h_n, c_n) = self.bottom_lstm_cell(inputs=temp_pred, states=[bottom_rnn_h0,bottom_rnn_h0])\n",
    "                    h_next,c_next = h_n, c_n\n",
    "                    temp_pred_pitch = self.bottom_lstm_pitch_dense(h_next)\n",
    "                    temp_pred_duration = self.bottom_lstm_duration_dense(h_next)\n",
    "                    temp_pred_step = self.bottom_lstm_step_dense(h_next)\n",
    "                    # think about normalization more here\n",
    "                    temp_pred = tf.concat([tf.cast(tf.expand_dims(tf.argmax(temp_pred_pitch,axis=1),axis=1)/128,dtype=tf.float32),temp_pred_duration,temp_pred_step],axis=1)\n",
    "                    subseq_pred = tf.expand_dims(temp_pred,axis=1)\n",
    "                else:\n",
    "                    if teacher_forcing:\n",
    "                        _, (h_n, c_n) = self.bottom_lstm_cell(inputs=subseq_x[:, j-1, :], states=[h_next,c_next])\n",
    "                    else:\n",
    "                        _, (h_n, c_n) = self.bottom_lstm_cell(inputs=temp_pred, states=[h_next,c_next])\n",
    "                    h_next,c_next = h_n, c_n\n",
    "                    temp_pred_pitch = self.bottom_lstm_pitch_dense(h_next)\n",
    "                    temp_pred_duration = self.bottom_lstm_duration_dense(h_next)\n",
    "                    temp_pred_step = self.bottom_lstm_step_dense(h_next)\n",
    "                    # think about normalization more here\n",
    "                    temp_pred = tf.concat([tf.cast(tf.expand_dims(tf.argmax(temp_pred_pitch,axis=1),axis=1)/128,dtype=tf.float32),temp_pred_duration,temp_pred_step],axis=1)\n",
    "                    subseq_pred = tf.concat([subseq_pred,tf.expand_dims(temp_pred,axis=1)],axis=1)\n",
    "\n",
    "\n",
    "            if subsec_idx==0:\n",
    "                pred = subseq_pred\n",
    "            else:\n",
    "                pred = tf.concat([pred,subseq_pred],axis=1)\n",
    "            \n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 64, 3), dtype=float32, numpy=\n",
       "array([[[ 0.6015625 , -0.02405372,  0.02968621],\n",
       "        [ 0.1640625 , -0.50718033,  0.2909191 ],\n",
       "        [ 0.1640625 , -0.65849984,  0.31725612],\n",
       "        ...,\n",
       "        [ 0.1640625 , -0.93025535,  0.14259909],\n",
       "        [ 0.1640625 , -0.9576364 ,  0.13737299],\n",
       "        [ 0.1640625 , -0.9365851 ,  0.12681685]],\n",
       "\n",
       "       [[ 0.984375  ,  0.01835872, -0.02815661],\n",
       "        [ 0.1640625 , -0.4081475 ,  0.22805421],\n",
       "        [ 0.1640625 , -0.61617005,  0.2970775 ],\n",
       "        ...,\n",
       "        [ 0.1640625 , -0.8472149 ,  0.07395652],\n",
       "        [ 0.1640625 , -0.957511  ,  0.083541  ],\n",
       "        [ 0.1640625 , -0.9403447 ,  0.07815933]],\n",
       "\n",
       "       [[ 0.3203125 , -0.02009549,  0.04322419],\n",
       "        [ 0.1640625 , -0.40743282,  0.21402963],\n",
       "        [ 0.1640625 , -0.6472487 ,  0.28843862],\n",
       "        ...,\n",
       "        [ 0.1640625 , -0.90947396,  0.10162848],\n",
       "        [ 0.1640625 , -0.9552187 ,  0.10269921],\n",
       "        [ 0.1640625 , -0.95545286,  0.10035466]],\n",
       "\n",
       "       [[ 0.453125  , -0.00533411,  0.01887234],\n",
       "        [ 0.1640625 , -0.39791474,  0.17870352],\n",
       "        [ 0.1640625 , -0.63092   ,  0.26101464],\n",
       "        ...,\n",
       "        [ 0.1640625 , -0.946129  ,  0.12843278],\n",
       "        [ 0.1640625 , -0.9470143 ,  0.12211256],\n",
       "        [ 0.1640625 , -0.94307905,  0.11552241]],\n",
       "\n",
       "       [[ 0.265625  , -0.00915134, -0.02146999],\n",
       "        [ 0.1640625 , -0.48947597,  0.2780736 ],\n",
       "        [ 0.1640625 , -0.678786  ,  0.33987343],\n",
       "        ...,\n",
       "        [ 0.1640625 , -0.9256945 ,  0.13040845],\n",
       "        [ 0.1640625 , -0.92884254,  0.12603252],\n",
       "        [ 0.1640625 , -0.9387865 ,  0.12375673]],\n",
       "\n",
       "       [[ 0.0078125 , -0.07974415,  0.0059912 ],\n",
       "        [ 0.84375   , -0.5009842 ,  0.29596153],\n",
       "        [ 0.1640625 , -0.69659793,  0.35761553],\n",
       "        ...,\n",
       "        [ 0.1640625 , -0.8995528 ,  0.11056744],\n",
       "        [ 0.1640625 , -0.9124952 ,  0.10171627],\n",
       "        [ 0.1640625 , -0.8680328 ,  0.08408497]]], dtype=float32)>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = MusicVAE_Decoder(4)\n",
    "\n",
    "z = tf.random.normal(shape=(6,256))\n",
    "decoder(z,temp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# RNN SAMPLING\n",
    "\n",
    "lstm_cell = LSTMCell(units=36)\n",
    "dense_last = Dense(units=3)\n",
    "\n",
    "h0 = tf.random.normal(shape=(temp_ds.shape[0],36))\n",
    "c0 = tf.random.normal(shape=(temp_ds.shape[0],36))\n",
    "\n",
    "h_next,c_next,temp_pred,out = None,None,None,None\n",
    "for j in range(0, temp_ds.shape[1]):\n",
    "    if j == 0:\n",
    "        _, (h_n, c_n) = lstm_cell(inputs=tf.zeros_like(temp_ds[:, j, :]), states=[h0,c0])\n",
    "        h_next,c_next = h_n, c_n\n",
    "        temp_pred = dense_last(h_next)\n",
    "        out = tf.expand_dims(temp_pred,axis=1)\n",
    "    else:\n",
    "        _, (h_n, c_n) = lstm_cell(inputs=temp_pred, states=[h_next,c_next])\n",
    "        h_next,c_next = h_n, c_n\n",
    "        temp_pred = dense_last(h_next)\n",
    "        out = tf.concat([out,tf.expand_dims(temp_pred,axis=1)],axis=1)\n",
    "\n",
    "\n",
    "# TEACHER FORCING\n",
    "\n",
    "lstm_cell = LSTMCell(units=36)\n",
    "dense_last = Dense(units=3)\n",
    "\n",
    "h0 = tf.random.normal(shape=(temp_ds.shape[0],36))\n",
    "c0 = tf.random.normal(shape=(temp_ds.shape[0],36))\n",
    "\n",
    "h_next,c_next,temp_pred,out = None,None,None,None\n",
    "for j in range(0, temp_ds.shape[1]):\n",
    "    if j == 0:\n",
    "        _, (h_n, c_n) = lstm_cell(inputs=tf.zeros_like(temp_ds[:, j, :]), states=[h0,c0])\n",
    "        h_next,c_next = h_n, c_n\n",
    "        temp_pred = dense_last(h_next)\n",
    "        out = tf.expand_dims(temp_pred,axis=1)\n",
    "    else:\n",
    "        _, (h_n, c_n) = lstm_cell(inputs=temp_ds[:, j-1, :], states=[h_next,c_next])\n",
    "        h_next,c_next = h_n, c_n\n",
    "        temp_pred = dense_last(h_next)\n",
    "        out = tf.concat([out,tf.expand_dims(temp_pred,axis=1)],axis=1)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional,LSTM,Dense\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "def get_encoder(latent_dim):\n",
    "    inputs = tf.keras.Input(shape = (EVENT_SIZE,3))\n",
    "    x = Bidirectional(LSTM(units=1024,return_sequences=True))(inputs)\n",
    "    x = Bidirectional(LSTM(units=1024,return_sequences=False))(x)\n",
    "    mu = Dense(units=latent_dim)(x)\n",
    "    rho = Dense(units=latent_dim)(x)\n",
    "    Encoder = tf.keras.Model(inputs=inputs,outputs=[mu,rho])\n",
    "    \n",
    "    return Encoder\n",
    "\n",
    "def get_decoder(latent_dim):\n",
    "    z = tf.keras.Input(shape = (latent_dim,))\n",
    "    x = tf.keras.layers.Dense(units=120, activation='relu')(z)\n",
    "    x = tf.keras.layers.Dense(units=500, activation='relu')(x)\n",
    "    decoded_img = tf.keras.layers.Dense(units=784)(x)\n",
    "    Decoder = tf.keras.Model(inputs=z,outputs=[decoded_img])\n",
    "    \n",
    "    return Decoder\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self,latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_block = get_encoder(latent_dim)\n",
    "        self.decoder_block = get_decoder(latent_dim)\n",
    "\n",
    "    def call(self,img):\n",
    "        z_mu,z_rho = self.encoder_block(img)\n",
    "\n",
    "        epsilon = tf.random.normal(shape=z_mu.shape,mean=0.0,stddev=1.0)\n",
    "        z = z_mu + tf.math.softplus(z_rho) * epsilon\n",
    "\n",
    "        decoded_img = self.decoder_block(z)\n",
    "\n",
    "        return z_mu,z_rho,decoded_img"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a73d1662bd3aab4de55a1a51be85519c6e25d5d617da76d142a49d5ef38ee143"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch_tf_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
