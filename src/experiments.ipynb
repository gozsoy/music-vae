{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import plot_piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# group according to bar length\n",
    "# do hidden state concat or not in decoder bottom lstm cell\n",
    "# MAKE DIMENSION OF X : 128 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG.YAML --NOT SAVED YET--\n",
    "\n",
    "# variable declarations\n",
    "\n",
    "TEMPO = 120.  # 1 sec = 2 beats\n",
    "SIXTEENTH_NOTE_BEATS = 0.25  # 1 16th note = 0.25 beats\n",
    "SIXTEENTH_NOTE_LEN = SIXTEENTH_NOTE_BEATS / (TEMPO / 60.)  # 1 16th note = 0.125 sec\n",
    "ONE_BAR_LEN = SIXTEENTH_NOTE_LEN * 16  # 16 16th notes = 1 bar = 2 sec\n",
    "FOUR_BAR_LEN = ONE_BAR_LEN * 4  # 16-bar = 32 sec\n",
    "\n",
    "EVENT_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS.PY --SAVED--\n",
    "\n",
    "def midi_notes2notes_df(notes):\n",
    "\n",
    "    prev_note = notes[0]\n",
    "\n",
    "    processed_notes = []\n",
    "    for temp_note in notes:\n",
    "        pitch = temp_note.pitch\n",
    "        duration = temp_note.end - temp_note.start\n",
    "        step = temp_note.start - prev_note.start\n",
    "        prev_note = temp_note\n",
    "\n",
    "        processed_notes.append({'pitch':pitch,'duration':duration,'step':step})\n",
    "\n",
    "    notes_df = pd.DataFrame.from_dict(processed_notes)\n",
    "\n",
    "    return notes_df\n",
    "\n",
    "\n",
    "def notesdf2midi_notes(notes_df):\n",
    "\n",
    "    recovered_midi_notes = []\n",
    "    current_step = 0.0\n",
    "    for _,row in notes_df.iterrows():\n",
    "        note_duration = row['duration']\n",
    "        current_step = current_step + row['step']\n",
    "        recovered_midi_notes.append(pretty_midi.Note(velocity=100,pitch=int(row['pitch']),start=current_step,end=current_step+note_duration))\n",
    "    \n",
    "    return recovered_midi_notes\n",
    "\n",
    "\n",
    "# reconstruction of recovered notes\n",
    "def pred_df2midi_file(notes_df):\n",
    "    recovered_notes = notesdf2midi_notes(notes_df.iloc[:64])\n",
    "\n",
    "    pm = pretty_midi.Instrument(program=0,is_drum=False)\n",
    "    pm.notes = recovered_notes\n",
    "\n",
    "    recovered_midi_file = pretty_midi.PrettyMIDI(initial_tempo=120.)\n",
    "    recovered_midi_file.instruments = [pm]\n",
    "    recovered_midi_file.time_signature_changes = [pretty_midi.TimeSignature(4,4,0.0)]\n",
    "    recovered_midi_file.write('../data/processed/mini_guitar.mid')\n",
    "\n",
    "    return\n",
    "\n",
    "def midi_data2tensor(midi_data):\n",
    "    notes = midi_data.instruments[0].notes\n",
    "    notes_df = midi_notes2notes_df(notes)\n",
    "    split_indices = np.arange(start=EVENT_SIZE,stop=len(notes_df),step=EVENT_SIZE)\n",
    "    single_batch_of_events = np.stack(np.split(notes_df.values,split_indices,axis=0)[:-1],axis=0)\n",
    "    return single_batch_of_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL.PY --SAVED--\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,LSTMCell\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "class MusicVAE_Encoder(tf.keras.Model):\n",
    "    def __init__(self,latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_lstm_1 = Bidirectional(LSTM(units=512,return_sequences=True))\n",
    "        self.encoder_lstm_2 = Bidirectional(LSTM(units=512,return_sequences=False))\n",
    "        self.mu_dense = Dense(units=latent_dim)\n",
    "        self.rho_dense = Dense(units=latent_dim)\n",
    "\n",
    "    def call(self,x):\n",
    "\n",
    "        x = self.encoder_lstm_1(x)\n",
    "        last_h = self.encoder_lstm_2(x)\n",
    "        mu = self.mu_dense(last_h)\n",
    "        rho = self.rho_dense(last_h)\n",
    "\n",
    "        return mu,rho\n",
    "\n",
    "\n",
    "class MusicVAE_Decoder(tf.keras.Model):\n",
    "    def __init__(self,conductor_len):\n",
    "        super().__init__()\n",
    "        self.conductor_len = conductor_len\n",
    "        self.conductor_dense = Dense(units=512,activation='tanh')\n",
    "        self.conductor_lstm_1 = LSTM(units=512,return_sequences=True)\n",
    "        self.conductor_lstm_2 = LSTM(units=256,return_sequences=True)\n",
    "        self.bottom_lstm_input_dense = Dense(units=128,activation='tanh')\n",
    "        self.bottom_lstm_cell = LSTMCell(units=128)\n",
    "        self.bottom_lstm_pitch_dense = Dense(units=128)\n",
    "        self.bottom_lstm_duration_dense = Dense(units=1)\n",
    "        self.bottom_lstm_step_dense = Dense(units=1)\n",
    "\n",
    "    def call(self,z,x,teacher_forcing=True): # teacher_forcing = False when prediction\n",
    "\n",
    "        # CONDUCTOR RNN\n",
    "        conductor_rnn_h0 = self.conductor_dense(z)\n",
    "        batch_size,_ = conductor_rnn_h0.shape\n",
    "        conductor_input = tf.zeros(shape=(batch_size,self.conductor_len,1))\n",
    "        conductor_output = self.conductor_lstm_1(inputs=conductor_input, initial_state=[conductor_rnn_h0, conductor_rnn_h0])\n",
    "        conductor_output = self.conductor_lstm_2(conductor_output)\n",
    "        bottom_input = self.bottom_lstm_input_dense(conductor_output) \n",
    "\n",
    "        # BOTTOM RNN\n",
    "        total_seq_len = x.shape[1]\n",
    "        subseq_len = int(total_seq_len/self.conductor_len)\n",
    "        \n",
    "        # model predictions\n",
    "        global_pitch_pred = None\n",
    "        global_duration_pred = None\n",
    "        global_step_pred = None\n",
    "\n",
    "        temp_pred = tf.zeros_like(x[:, 0, :])\n",
    "        for subsec_idx in range(self.conductor_len):\n",
    "            bottom_rnn_h0 = bottom_input[:,subsec_idx,:]\n",
    "            subseq_x = x[:,subsec_idx*subseq_len:(subsec_idx+1)*subseq_len,:]\n",
    "\n",
    "            h_next,c_next = None,None\n",
    "            for j in range(0, subseq_x.shape[1]):\n",
    "                if j == 0:\n",
    "                    if teacher_forcing:\n",
    "                        _, (h_n, c_n) = self.bottom_lstm_cell(inputs=tf.zeros_like(subseq_x[:, j, :]), states=[bottom_rnn_h0,bottom_rnn_h0])\n",
    "                    else:\n",
    "                        _, (h_n, c_n) = self.bottom_lstm_cell(inputs=temp_pred, states=[bottom_rnn_h0,bottom_rnn_h0])\n",
    "                    h_next,c_next = h_n, c_n\n",
    "                    # 3 different outputs from model\n",
    "                    temp_pred_pitch = self.bottom_lstm_pitch_dense(h_next)\n",
    "                    temp_pred_duration = self.bottom_lstm_duration_dense(h_next)\n",
    "                    temp_pred_step = self.bottom_lstm_step_dense(h_next)\n",
    "                    # prepare next step's input by merging these 3 different outputs\n",
    "                    # think about normalization more here\n",
    "                    temp_pred = tf.concat([tf.cast(tf.expand_dims(tf.argmax(temp_pred_pitch,axis=1),axis=1)/128,dtype=tf.float32),temp_pred_duration,temp_pred_step],axis=1)\n",
    "                    \n",
    "                    # save each step's 3 different outputs for loss computation\n",
    "                    if subsec_idx==0:\n",
    "                        global_pitch_pred = tf.expand_dims(temp_pred_pitch,axis=1)\n",
    "                        global_duration_pred = tf.expand_dims(temp_pred_duration,axis=1)\n",
    "                        global_step_pred = tf.expand_dims(temp_pred_step,axis=1)\n",
    "                    else:\n",
    "                        global_pitch_pred = tf.concat([global_pitch_pred,tf.expand_dims(temp_pred_pitch,axis=1)],axis=1)\n",
    "                        global_duration_pred = tf.concat([global_duration_pred,tf.expand_dims(temp_pred_duration,axis=1)],axis=1)\n",
    "                        global_step_pred = tf.concat([global_step_pred,tf.expand_dims(temp_pred_step,axis=1)],axis=1)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    if teacher_forcing:\n",
    "                        _, (h_n, c_n) = self.bottom_lstm_cell(inputs=subseq_x[:, j-1, :], states=[h_next,c_next])\n",
    "                    else:\n",
    "                        _, (h_n, c_n) = self.bottom_lstm_cell(inputs=temp_pred, states=[h_next,c_next])\n",
    "                    h_next,c_next = h_n, c_n\n",
    "                    # 3 different outputs from model\n",
    "                    temp_pred_pitch = self.bottom_lstm_pitch_dense(h_next)\n",
    "                    temp_pred_duration = self.bottom_lstm_duration_dense(h_next)\n",
    "                    temp_pred_step = self.bottom_lstm_step_dense(h_next)\n",
    "                    # prepare next step's input by merging these 3 different outputs\n",
    "                    # think about normalization more here\n",
    "                    temp_pred = tf.concat([tf.cast(tf.expand_dims(tf.argmax(temp_pred_pitch,axis=1),axis=1)/128,dtype=tf.float32),temp_pred_duration,temp_pred_step],axis=1)\n",
    "\n",
    "                    # save each step's 3 different outputs for loss computation\n",
    "                    global_pitch_pred = tf.concat([global_pitch_pred,tf.expand_dims(temp_pred_pitch,axis=1)],axis=1)\n",
    "                    global_duration_pred = tf.concat([global_duration_pred,tf.expand_dims(temp_pred_duration,axis=1)],axis=1)\n",
    "                    global_step_pred = tf.concat([global_step_pred,tf.expand_dims(temp_pred_step,axis=1)],axis=1)\n",
    "\n",
    "        return global_pitch_pred,global_duration_pred,global_step_pred\n",
    "\n",
    "\n",
    "\n",
    "class MusicVAE(tf.keras.Model):\n",
    "    def __init__(self,latent_dim,conductor_len,teacher_forcing):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.conductor_len = conductor_len\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        self.encoder = MusicVAE_Encoder(latent_dim)\n",
    "        self.decoder = MusicVAE_Decoder(conductor_len)\n",
    "\n",
    "    def call(self,input_seq):\n",
    "        z_mu,z_rho = self.encoder(input_seq)\n",
    "\n",
    "        epsilon = tf.random.normal(shape=z_mu.shape,mean=0.0,stddev=1.0)\n",
    "        z = z_mu + tf.math.softplus(z_rho) * epsilon\n",
    "\n",
    "        global_pitch_pred,global_duration_pred,global_step_pred = self.decoder(z,input_seq,self.teacher_forcing)\n",
    "\n",
    "        return z_mu,z_rho,global_pitch_pred,global_duration_pred,global_step_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=3'>4</a>\u001b[0m complete_batch_of_events \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m midi_file \u001b[39min\u001b[39;00m midi_list:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=6'>7</a>\u001b[0m     midi_data \u001b[39m=\u001b[39m pretty_midi\u001b[39m.\u001b[39mPrettyMIDI(midi_file)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=7'>8</a>\u001b[0m     single_batch_of_events \u001b[39m=\u001b[39m midi_data2tensor(midi_data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/self-projects/music-vae/src/experiments.ipynb#ch0000084?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m complete_batch_of_events \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py:69\u001b[0m, in \u001b[0;36mPrettyMIDI.__init__\u001b[0;34m(self, midi_file, resolution, initial_tempo)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=66'>67</a>\u001b[0m     tick \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=67'>68</a>\u001b[0m     \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m track:\n\u001b[0;32m---> <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=68'>69</a>\u001b[0m         event\u001b[39m.\u001b[39mtime \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tick\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=69'>70</a>\u001b[0m         tick \u001b[39m=\u001b[39m event\u001b[39m.\u001b[39mtime\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/pretty_midi/pretty_midi.py?line=71'>72</a>\u001b[0m \u001b[39m# Store the resolution for later use\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DATASET.PY --NOT SAVED YET-- or preprocessing.py outside model definition\n",
    "\n",
    "import glob\n",
    "midi_list = glob.glob(\"../data/maestro-v3.0.0/2018/*.midi\")\n",
    "\n",
    "complete_batch_of_events = None\n",
    "\n",
    "for midi_file in midi_list:\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    single_batch_of_events = midi_data2tensor(midi_data)\n",
    "    if complete_batch_of_events is None:\n",
    "        complete_batch_of_events = single_batch_of_events\n",
    "    else:\n",
    "        complete_batch_of_events= np.concatenate((complete_batch_of_events,single_batch_of_events),axis=0)\n",
    "\n",
    "print(complete_batch_of_events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(complete_batch_of_events)\n",
    "train_ds = train_ds.shuffle(1000).batch(6)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_ds in train_ds:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "CONDUCTOR_LEN = 4\n",
    "\n",
    "model = MusicVAE(latent_dim=latent_dim,conductor_len=CONDUCTOR_LEN,teacher_forcing=True)\n",
    "\n",
    "z_mu,z_rho,global_pitch_pred,global_duration_pred,global_step_pred = model(temp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6, 64, 1])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step_pred.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a73d1662bd3aab4de55a1a51be85519c6e25d5d617da76d142a49d5ef38ee143"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch_tf_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
